\section{Complete Traceability}
\label{sec:discussion}

Through formal definitions in the previous section we highlight two critical facets -- the semantics and completeness -- in establishing traceability. In our experience, we found that just being able to trace a requirement to a target artifact that satisfies it, say a line of code, is not useful; a holistic view of how that line of code in conjunction with other related lines of code satisfy the requirement provides meaningful information to perform analysis such as assessing the impact of a change. By defining a trace from requirements to the set of target artifacts, we advocate that traceability be captured in a way that upholds its semantic rationale. Further, we also found that most analyses that are performed using the established traceability without a clear picture of the actual adequacy of traces is not, in reality, useful and trustworthy. By defining a complete trace for every requirement, we recapitulate that it is critical to assess the thoroughness of the established traces. In this section, we elaborate on how rethinking about these facets helps understand, assess and use traceability precisely.

%that we explain in the previous section not only provides an in-depth understanding of how the system satisfies requirements but acts as a metric to assess the quality of traceability established for a system and enhances the precision of analysis using the traceability information.

\subsection{Categorizing the Set of Support}

Establishing $CRST_m$ for a requirement, one gets a clear picture of the all possible ways that requirement is satisfied. This information helps categorize each target artifact into one of the following groups for that requirement ($r\in R$), also illustrated using a Venn diagram in Figure~\ref{fig:maymust},\anitha{not using the figure enough during explanations}
\begin{itemize}
  \item \textbf{MUST} elements - the target artifacts that are present in all the sets of support for a requirement.
      %$$ MUST_x = \{\forall i (S_xi \in \Sigma_x) \mid \bigcap S_xi \}$$
      $$ MUST (r) = \{\bigcap CRST_m (r) \}$$

  \item \textbf{MAY} elements - target artifacts in some sets of support, but not all.
      $$MAY(r) = \{ CRST_m (r) \setminus MUST (r) \}$$

  \item \textbf{IRRELEVANT} elements - target artifacts that are not in any of the set of support. $$IRR_x = \{I \setminus CRST_m (r)\}$$
\end{itemize}

\begin{figure}[htb]
\includegraphics[width=\columnwidth]{images/may_must.pdf}
\caption{May - Must - Irrelevant Set of Support}\label{fig:maymust}
\end{figure}

This categorization helps identify the role and relevance of each target artifact in satisfying a requirement. The MUST elements are those target artifacts that are absolutely necessary for the requirement satisfaction. Hence, any change to these elements will most likely impact on each other. On the other hand, MAY elements indicate those target artifacts that satisfy the requirement in one of the possible ways. Any change to just one of these elements will not affect the satisfaction of that requirement. The IRRELEVANT elements are not related to satisfying the requirements, hence neither a change in them affects the requirement satisfaction, nor does a change in requirement induce a change in these elements.


\subsection{Using Traces for Precise Analysis}

The categorization of the set of support to be useful in several types of analysis.

\paragraph{Impact Analysis}: It helps understand how a change in the requirement change will affect the target artifacts and vice versa. This categorization precisely helps identify those parts of the system that definitely have to change when there is a change in requirement. The MUST elements are those target artifacts that are highly likely to change with any change in the requirement, whereas not all MAY elements need to be changed to satisfy the requirement. Obviously, one need not even worry about the IRRELEVANT elements. If we take all the sets of support for all the requirements and then categorize them, then the elements in the MUST set points to target artifacts that are critical for every requirement. This helps assess how a change in one element may affect other requirements.

Reversing the direction of trace, if we now, take each target artifact and trace to the requirements it satisfies, one can get clear idea about all the requirements it satisfies. If we decide to change a target artifact as long as it is not in any of the MUST sets for any requirement, it becomes evident that it will not affect the satisfaction of any requirement.

\paragraph{Verification and Validation}

This categorization helps tailor the Verification and Validation in systems. For instance, if most requirements have a certain target artifact in their MUST set, say an particular assumption, it reveals the criticality of focusing attention to validate that artifact. Along the same lines, for a system with a complex architecture (components that each have functionality) such as  system of systems, this categorization helps identify components that is critical to satisfy most requirements. This helps plan verification strategies.

Further, the notion of getting complete traces helps access if the requirements are satisfied by the system in an unintended manner. It is well known that issues such as vacuity~\cite{Kupferman03:Vacuity} can cause requirements to be satisfied in a trivial manner. Even for non-vacuous requirements, it is possible to over-constrain the {\em environment} or unintentionally added functionality in the system that caused the requirements to be satisfied. By capturing all the set of support and categorizing them, one could evaluate if there are unintended target artifacts that might cause the requirements to be satisfied. %In one of our experiments using an infusion pump system software, while capturing the $RST_m$ for a requirement we discovered that there was a requirement whose set of support included a not the one that  to be trivially satisfied. When we

\paragraph{Completeness Checking}

By getting all the sets of support for all requirements of the system and categorizing them,  it gives a clear picture about the role of target artifacts in satisfying the requirement. By reversing the direction of the complete requirement satisfaction trace, one can find if there are target artifacts that do not trace to any requirement. This is a possible indication of ``gold plating" or missing requirements. In other words, it helps assess if there requirements of the system are completely specified with respect to describing all the behaviors of the system. Being able to assess the coverage of requirements over the model is crucial in the safety critical system domain.

\paragraph{Bench marking}
\anitha{I am not happy with this now.... But I do want to say something along these lines... }
To the best of our knowledge, the existing benchmarks and ways to establish benchmarks to assess the effectiveness of traceability techniques or evaluate the trace links is not guaranteed to be flawless. The correctness and thoroughness of the benchmarks are in someway assumed and not rigorously assessed. A number of research work compute the precision and recall of their traceability approaches by using such benchmarks. We believe that to evaluate the completeness and precision of those benchmarks we need to first rigorously characterize it. The requirements satisfaction trace defined in the previous section helps us move forward in that direction. Since the trace is based on establishing a satisfaction argument it is straightforward to verify/validate the precision of each trace (which is 100$\%$). The complete requirements satisfaction trace, as the name suggests, is defined to capture traces to all sets of support. While this may sound theoretical, once established it will be the perfect benchmark to compare results to.

Contrary to the general notion that it is impossible or extraordinarily difficult to identify all trace link in practice~\cite{stravsunskas2002traceability}, some of the initial results from our recent efforts~\cite{IVCTechReport} indicate that such complete requirements satisfaction trace can be established, in fact automatically in the realm of formal methods and model based developments. In the next section, we breifely describe our prior work and our plans to extend it to establish complete traceability. While the details the approach and initial results are not in scope of this paper, an interested reader is directed to~\cite{IVCTechReport}.



%
%
%
%\newcommand{\bfalg}{IVC\_BF}
%\newcommand{\ucalg}{IVC\_UC}
%\newcommand{\ucbfalg}{IVC\_UCBF}
%
%The approach from the previous section establishes trace links between properties at different levels of the system architecture based on inductive validity cores derived from inductive proofs.  For functional requirements, this approach automates the traceability information necessary to establish {\em satisfaction arguments} in the style of \mike{Ref to Will it Work HERE}.  In so doing, it allows a substantial change in how traceability is performed, and to some extent, what traceability {\em means}.  As we explained in the motivating example, there is often more than one way that the requirements could be satisfied by the model, that is, there is more than one set of support for the requirement.  This requires an examination of how we perceive traceability in terms of which model elements (equivalently: subcomponent properties) are {\em always required} by any proof, which elements are {\em sometimes required}, and which elements are never required, which we call {\em must} and {\em may} trace links.  It also brings up fruitful discussions on how architectures are constructed and the nature of the links between requirements and architecture.
%
%\subsection{Overhead of Computing IVCs and Minimality of Generated IVCss}
%In our accompanying technical report~\cite{}, we present two algorithms that are used to compute IVCs and an experiment to examine the quality of the approach.  Our primary concerns are the {\em efficiency} and {\em
%minimality} of the approach.  Efficiency is computed in terms of wall-clock time: how much overhead does the IVC algorithm introduce?  Minimality is determined by the size of the IVC: cores with a smaller number of
%variables are preferred to cores with a larger number of variables.  Both of the proposed algorithms are {\em sound}, in the sense that from the IVC, it is always possible to reconstruct the proof, so they do not miss trace links.
%
%In~\cite{}, we present two algorithms that pose different tradeoffs between these goals.  The first algorithm, called \ucalg, uses UNSAT cores to determine an IVC and runs very efficiently (on average, for the 477 models in the experiment, about 15\% overhead over performing the proof when using the z3~\cite{} solver), but is not guaranteed to yield a minimal IVC; on average, it yields a core that is 21\% larger than minimal.  The second algorithm, called \ucbfalg, adds a ``brute force'' postprocessing stage that guarantees that the generated core is minimal, but adds, on average, a greater than 30-times slowdown to the proof.
%
%\subsection{On-Demand Traceability}
%
%Because the efficient algorithm (\ucalg) adds very little overhead to the model-checking process, which is itself quite efficient, we can think of it as providing ``on-demand'' traceability.  In most traceability approaches, maintenance of trace links is a very significant concern and one of the reasons that finding and fixing errors is so expensive in critical systems.  However, with our approach, it is straightforward to provide users with accurate trace information consistent with the model immediately after changes are made.  This allows traceability to easily be used as an {\em exploratory} analysis early during requirements gathering and architectural prototyping.  We have used it in this way in the DARPA SOSITE project, where we used traceability information to check whether we were writing appropriate requirements and to check ``semantic vacuity'', as will be described in a following section.
%
%In hand-generated trace links for satisfaction, there is a potential for both missing required traceability links and addition unnecessary traceability links.  In our approach, there is an interesting tradeoff in terms of usability of trace links: is it more important that they are inexpensive to generate or that they are minimal?  In our approach, for functional requirements, if we overapproximate the set of required model elements, does this still provide value?  Our claim of (on average) 21\% larger sets of support is perhaps misleading: for the GPCA models, which contain hundreds of possible subcomponent requirements that contribute to a proof, the \ucalg\ set of support is either the same as the \ucbfalg, or it has between 1 and 5 extra elements (e.g., 10 elements vs. 8 elements).  For users seeking to understand the relationships between system and subcomponent requirements, it seems that the faster algorithm may yield reasonable results.  On the other hand, if we want to use the tool for some kind of certification-related activity, it is likely worth the extra cost to get more exact results.
%
%\mike{more here???}

%\subsection{Categorizing Trace Links}
%We often describe ``the'' traceability matrix for a system, tracing requirements to lower-level requirements, design artifacts, or implementations.  However, at least for functional requirements, it is more correct to speak of ``a'' traceability matrix, because there are multiple minimal sets of model elements that lead to a proof.
%
%
%For instance, in safety critical system domain for fault tolerance one could intentionally add multiple ways to satisfy a requirement, or one could unintentionally add model functionality that leads to multiple ways of satisfying a requirement. By iteratively using our approach to query the solver for different sets of support until it does not find any, we can obtain all the sets of support for a requirement. Getting all the sets of support is nothing but establishing complete traceability between the requirements and all the parts of the model that contribute to satisfying it.
%
%%\subsection{Categorizing Support Elements}
%
%Establishing complete traceability paves way to indepth understanding of the model. Given all sets of support for a requirement, one could categorize them into the following, as illustrated in Figure~\ref{fig:maymust}:
%\begin{itemize}
%  \item MUST elements - support elements that are present in all the sets of support
%  \item MAY elements -  elements in set of support, but not common to all the sets of support
%  \item IRRELEVANT elements -  support elements of the model that are not in any of the set of support
%\end{itemize}
%
%\begin{figure}[htb]
%\includegraphics[width=\columnwidth]{images/may_must.pdf}
%\caption{May - Must - Irrelevant Set of Support}\label{fig:maymust}
%\end{figure}
%
%The MUST elements indicate that those parts of the model are absolutely necessary for the requirement to be satisfied. Hence, any change to either requirements or one of these elements will have an impact on the other. On the other hand, MAY elements indicate the parts of the model that satisfy the requirement in one of the possible ways. Any change to just one of these elements will not affect the satisfaction of the requirements. The NOT elements are in no way related to satisfying the requirements. When performing impact analysis of a change in requirements or dependency analysis to assess the effect of changing the model, this traceability provides a complete picture of all the dependencies.
%
%
%



%
%
%\subsection{Traceability Coverage}
%
%While establishing complete traceability is useful, to the best of our knowledge, how elaborately should traceability be captured is not well discussed in the literature. The question is "Should traceability be quantified?" The usual trace relationships between the system (or its model) and the requirements is "satisfies" or "implements" etc. We recommend that the relationship explicitly state whether traceability captures "one way to satisfy/implement" or "all the ways to satisfy/implement" the requirements. Making it explicit helps precisely realize the benefits of traceability.
%
%\anitha{in progress.}....

